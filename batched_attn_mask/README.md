This contains the implementation of a 3d attn_mask using the TransformerEncoderLayer in pytorch. This was copied from https://github.com/pytorch/pytorch/pull/31996/files.

Notably, at the time of writing this, the Transformer modules in pytorch do not generate positional embeddings.
